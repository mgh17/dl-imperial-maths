{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Implement a CNN classifier in Tensorflow.\n",
    "### - Experiment with batch normalisation, dropout and residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgh17/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "### Import and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape) # training set images\n",
    "print(y_train.shape) # training set labels\n",
    "print(x_test.shape)  # test set images\n",
    "print(y_test.shape)  # test set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "\n",
    "print(n_train)\n",
    "print(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182b184908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 3\n"
     ]
    }
   ],
   "source": [
    "example = np.random.choice(np.arange(n_train))\n",
    "\n",
    "image = x_train[example]\n",
    "label = y_train[example]\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "print(\"Digit: {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the images to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(images):\n",
    "    return images*255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_gs = convert_to_grayscale(x_train)\n",
    "test_set_gs = convert_to_grayscale(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(training_set_gs.shape)\n",
    "print(test_set_gs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x184919c0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 8\n"
     ]
    }
   ],
   "source": [
    "example = np.random.choice(np.arange(n_train))\n",
    "\n",
    "image = training_set_gs[example]\n",
    "label = y_train[example]\n",
    "\n",
    "if label == 10:\n",
    "    label = 0\n",
    "\n",
    "plt.imshow(np.squeeze(image), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"Digit: {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the labels as one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels):\n",
    "    \n",
    "    labels = np.squeeze(labels)\n",
    "    one_hot_labels = []\n",
    "    for num in labels:\n",
    "        one_hot = [0.0] * 10\n",
    "        one_hot[num] = 1.0\n",
    "        one_hot_labels.append(one_hot)\n",
    "    labels = np.array(one_hot_labels).astype(np.float32)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_one_hot = one_hot(y_train)\n",
    "test_labels_one_hot = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(training_labels_one_hot.shape)\n",
    "print(test_labels_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "### Encode the labels as one-hot vectors\n",
    "\n",
    "def one_hot(labels):\n",
    "    \n",
    "    labels = np.squeeze(labels)\n",
    "    one_hot_labels = []\n",
    "    for num in labels:\n",
    "        one_hot = [0.0] * 10\n",
    "        one_hot[num] = 1.0\n",
    "        one_hot_labels.append(one_hot)\n",
    "    labels = np.array(one_hot_labels).astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "training_labels_one_hot = one_hot(y_train)\n",
    "test_labels_one_hot = one_hot(y_test)\n",
    "\n",
    "print(training_labels_one_hot.shape)\n",
    "print(test_labels_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN:\n",
    "    def __init__(self, wd_factor, learning_rate):\n",
    "        self.wd_factor = wd_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_pointer = 0\n",
    "        self.test_pointer = 0\n",
    "        \n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28], name='input')\n",
    "        self.ground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='ground_truth')\n",
    "        \n",
    "        # For batch norm and dropout\n",
    "        self.is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        print(self.input)\n",
    "        \n",
    "        self._build_graph()\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        weights = []  # for weight decay\n",
    "        \n",
    "        with tf.variable_scope('layers'):\n",
    "            h = tf.layers.conv2d(self.input, 32, (11, 11), strides=(3, 3), padding='same', \n",
    "                                 data_format='channels_last', activation=None, use_bias=True,\n",
    "                                 kernel_initializer=tf.glorot_uniform_initializer(), name='conv1')\n",
    "            print(h)\n",
    "            \n",
    "            h = tf.layers.batch_normalization(h, training=self.is_training)\n",
    "            h = tf.nn.relu(h)\n",
    "            h = tf.layers.conv2d(h, 64, (5, 5), strides=(1, 1), padding='same', \n",
    "                                 data_format='channels_last', activation=None, use_bias=True,\n",
    "                                 kernel_initializer=tf.glorot_uniform_initializer(), name='conv2')\n",
    "            \n",
    "            h = tf.layers.batch_normalization(h, training=self.is_training)\n",
    "            h = tf.nn.relu(h)\n",
    "            h = tf.layers.conv2d(h, 64, (3, 3), strides=(1, 1), padding='same', \n",
    "                                 data_format='channels_last', activation=None, use_bias=True,\n",
    "                                 kernel_initializer=tf.glorot_uniform_initializer(), name='conv3')\n",
    "            \n",
    "            # Downsample\n",
    "            h = tf.layers.max_pooling2d(h, (2, 2), (2, 2), padding='valid', name='pool1')\n",
    "            print(h)\n",
    "            \n",
    "            # Fully connected layers\n",
    "            h = tf.layers.batch_normalization(h, training=self.is_training)\n",
    "            h = tf.nn.relu(h)\n",
    "            h = tf.layers.flatten(h)\n",
    "            print(h)\n",
    "            \n",
    "            h = tf.layers.dense(h, 32, kernel_initializer=tf.glorot_uniform_initializer(), \n",
    "                                activation=tf.nn.relu, name='dense1')\n",
    "            print(h)\n",
    "            h = tf.layers.dropout(h, rate=0.25, training=self.is_training, name='dropout1')\n",
    "            print(h)\n",
    "            \n",
    "            self.logits = tf.layers.dense(h, 10, kernel_initializer=tf.glorot_uniform_initializer(), \n",
    "                                          activation=tf.identity, name='dense2')\n",
    "            print(self.logits)\n",
    "            self.prediction = tf.nn.softmax(self.logits, name='softmax_prediction')\n",
    "            \n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, \n",
    "                                                                                  labels=self.ground_truth))\n",
    "            self.loss += self.weight_decay()\n",
    "            \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.train_op = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "    def weight_decay(self):\n",
    "        loss = 0\n",
    "        for v in tf.global_variables():\n",
    "            if 'Adam' in v.name:\n",
    "                continue\n",
    "            elif 'kernel' in v.name:\n",
    "                loss += self.wd_factor * tf.nn.l2_loss(v)\n",
    "        print(loss)\n",
    "        return loss\n",
    "    \n",
    "    def train_minibatch(self, samples, labels, batch_size):\n",
    "        if self.train_pointer + batch_size <= samples.shape[0]:\n",
    "            samples_minibatch = samples[self.train_pointer: self.train_pointer + batch_size]\n",
    "            labels_minibatch = labels[self.train_pointer: self.train_pointer + batch_size]\n",
    "            self.train_pointer += batch_size\n",
    "        else:\n",
    "            samples_minibatch = samples[self.train_pointer:]\n",
    "            labels_minibatch = labels[self.train_pointer: self.train_pointer + batch_size]\n",
    "            self.train_pointer = 0\n",
    "        return samples_minibatch, labels_minibatch\n",
    "\n",
    "    def train(self, train_samples, train_labels, train_batch_size, iteration_steps):\n",
    "        print('Start Training')\n",
    "        losses = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            for i in range(iteration_steps):\n",
    "                samples, labels = self.train_minibatch(train_samples, train_labels, train_batch_size)\n",
    "                \n",
    "                feed_dict = {self.input: samples, self.ground_truth: labels, self.is_training: True}\n",
    "                _, loss = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "                \n",
    "                if i % 50 == 0:\n",
    "                    print(\"Minibatch loss at step {}: {}\".format(i, loss))\n",
    "                    losses.append([i, loss])\n",
    "                    \n",
    "            saver.save(sess, './model')\n",
    "        return losses\n",
    "                    \n",
    "    def test_minibatch(self, samples, labels, batch_size):\n",
    "        if self.test_pointer + batch_size <= samples.shape[0]:\n",
    "            samples_minibatch = samples[self.test_pointer: self.test_pointer + batch_size]\n",
    "            labels_minibatch = labels[self.test_pointer: self.test_pointer + batch_size]\n",
    "            self.test_pointer += batch_size\n",
    "            end_of_epoch = False\n",
    "        else:\n",
    "            samples_minibatch = samples[self.test_pointer:]\n",
    "            labels_minibatch = labels[self.test_pointer: self.test_pointer + batch_size]\n",
    "            self.test_pointer = 0\n",
    "            end_of_epoch = True\n",
    "        return samples_minibatch, labels_minibatch, end_of_epoch\n",
    "            \n",
    "    def test(self, test_samples, test_labels, test_batch_size):\n",
    "        self.test_pointer = 0\n",
    "        end_of_epoch = False\n",
    "        losses = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.import_meta_graph(\"./model.meta\")\n",
    "            saver.restore(sess, './model')\n",
    "            while not end_of_epoch:\n",
    "                samples, labels, end_of_epoch = self.test_minibatch(test_samples, test_labels, test_batch_size)\n",
    "                feed_dict = {self.input: samples, self.ground_truth: labels, self.is_training: False}\n",
    "                losses.append(sess.run(self.loss, feed_dict=feed_dict))  \n",
    "        print(\"Average test loss: {}\".format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input:0\", shape=(?, 28, 28), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-70b7e7f3a588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mWD_FACTOR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNIST_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWD_FACTOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-d78588e26744>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, wd_factor, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d78588e26744>\u001b[0m in \u001b[0;36m_build_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m             h = tf.layers.conv2d(self.input, 32, (11, 11), strides=(3, 3), padding='same', \n\u001b[1;32m     22\u001b[0m                                  \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                  kernel_initializer=tf.glorot_uniform_initializer(), name='conv1')\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    423\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       _scope=name)\n\u001b[0;32m--> 425\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \"\"\"\n\u001b[0;32m--> 805\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1422\u001b[0m                            \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m                            str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m   1425\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 28, 28]"
     ]
    }
   ],
   "source": [
    "WD_FACTOR = 0.0\n",
    "LEARNING_RATE = 0.001\n",
    "model = MNIST_CNN(WD_FACTOR, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 128\n",
    "ITERATIONS = 100\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "losses = model.train(training_set_gs, training_labels_one_hot, TRAIN_BATCH_SIZE, ITERATIONS)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Training time: {}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    losses = np.array(losses)\n",
    "    np.save('./train_losses.npy', losses)\n",
    "    print(losses.shape)\n",
    "except NameError:\n",
    "    losses = np.load('./train_losses.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = losses[:, 0]\n",
    "train_loss = losses[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, train_loss, 'b-')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 128\n",
    "\n",
    "model.test(test_set_gs, test_labels_one_hot, TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.random.choice(np.arange(n_test))\n",
    "\n",
    "sample = np.expand_dims(test_set_gs[example], axis=0)\n",
    "label = np.expand_dims(test_labels_one_hot[example], axis=0)\n",
    "\n",
    "digit = np.where(label[0]==1.0)[0][0]\n",
    "\n",
    "feed_dict = {model.input: sample, model.ground_truth: label, model.is_training: False}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(\"./model.meta\")\n",
    "    saver.restore(sess, './model')\n",
    "    prediction = sess.run(model.prediction, feed_dict=feed_dict)[0]\n",
    "\n",
    "image = np.reshape(sample, (32, 32))\n",
    "\n",
    "print(\"Test sample digit: {}\".format(digit))\n",
    "fig, ax = plt.subplots(1, 2, figsize=(17, 5))\n",
    "ax[0].imshow(image, cmap='gray')\n",
    "ax[0].set_title(\"Test example\")\n",
    "\n",
    "classes = np.arange(10)\n",
    "width = 1.0\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "ax[1].bar(classes, prediction, width, color='Blue')\n",
    "ax[1].set_ylabel('Probabilities')\n",
    "ax[1].set_title('Network categorical distribution')\n",
    "ax[1].set_xticks(classes)\n",
    "ax[1].set_xticklabels(('0', '1', '2', '3', '4', '5', '6', '7', '8', '9'))\n",
    "ax[1].set_xlabel('Digit class')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Network prediction probabilities:\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
